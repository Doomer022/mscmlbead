{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c269e8e6",
   "metadata": {},
   "source": [
    "# CELL 1 - Initialization"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd052f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:14:02.403199900Z",
     "start_time": "2025-12-12T17:13:59.668813700Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import optuna\n",
    "import random\n",
    "import kagglehub # Import for the new download method\n",
    "\n",
    "# --- Constants and Setup ---\n",
    "\n",
    "IMAGE_SIZE = 100 \n",
    "BATCH_SIZE = 64\n",
    "RANDOM_SEED = 42\n",
    "NUM_EPOCHS_TUNE = 5 # Fewer epochs for quick HPO search\n",
    "NUM_EPOCHS_FINAL = 50 # Max epochs for final training (with Early Stopping)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Determine device (CUDA for GPU training)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "\n",
    "# --- Kaggle Download and DATA_DIR Setup ---\n",
    "DATASET_ID = \"utkarshsaxenadn/fruits-classification\"\n",
    "\n",
    "print(f\"\\nDownloading Kaggle dataset ID: {DATASET_ID} using kagglehub...\")\n",
    "print(\"NOTE: This requires your Kaggle API key (kaggle.json) to be set up in ~/.kaggle/.\")\n",
    "\n",
    "try:\n",
    "    # Use kagglehub to download the dataset\n",
    "    path = kagglehub.dataset_download(DATASET_ID)\n",
    "    \n",
    "    # kagglehub returns the path to the root of the extracted dataset files\n",
    "    DATA_DIR = str(path)\n",
    "    \n",
    "    print(\"Download complete.\")\n",
    "    print(f\"Path to dataset files: {DATA_DIR}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Kaggle Download Failed. Error: {e}\")\n",
    "    print(\"Please ensure your Kaggle API key is correctly configured and the kernel is switched.\")\n",
    "    # Fallback path if the download fails for manual placement\n",
    "    DATA_DIR = './fruits-classification' \n",
    "    print(f\"Falling back to assumed manual data path: {DATA_DIR}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA device name: NVIDIA GeForce RTX 3060\n",
      "\n",
      "Downloading Kaggle dataset ID: utkarshsaxenadn/fruits-classification using kagglehub...\n",
      "NOTE: This requires your Kaggle API key (kaggle.json) to be set up in ~/.kaggle/.\n",
      "Download complete.\n",
      "Path to dataset files: C:\\Users\\Bence\\.cache\\kagglehub\\datasets\\utkarshsaxenadn\\fruits-classification\\versions\\1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b79c0c22",
   "metadata": {},
   "source": [
    "# CELL 2 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "02e6cca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:14:02.469689600Z",
     "start_time": "2025-12-12T17:14:02.405201600Z"
    }
   },
   "source": [
    "# --- Data Transforms, Normalization, and Features (Data Augmentation) ---\n",
    "\n",
    "# 1. PIL Image Transforms (Augmentation applied before Tensor conversion)\n",
    "pil_transforms = [\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomRotation(15), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "]\n",
    "\n",
    "# 2. Tensor Conversion\n",
    "tensor_conversion = [\n",
    "    transforms.ToTensor(),\n",
    "]\n",
    "\n",
    "# 3. Tensor Transforms (Augmentation and Normalization applied after Tensor conversion)\n",
    "tensor_transforms = [\n",
    "    # FIX: RandomErasing MUST be after ToTensor()\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3)), \n",
    "    # Normalization\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "]\n",
    "\n",
    "# Training Transforms (All steps)\n",
    "train_transforms = transforms.Compose(pil_transforms + tensor_conversion + tensor_transforms)\n",
    "\n",
    "# Validation/Test Transforms (Only resize, ToTensor, Normalization)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "\n",
    "# --- Adatfolyam (ImageFolder Ã©s DataLoader) ---\n",
    "TRAIN_DIR = DATA_DIR + '\\\\Fruits Classification\\\\train'\n",
    "TEST_DIR = DATA_DIR + '\\\\Fruits Classification\\\\test'\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(TEST_DIR, transform=test_transforms)\n",
    "\n",
    "# Split training data into training and validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(train_data))\n",
    "val_size = len(train_data) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders (Data Streaming)\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "CLASS_NAMES = train_data.classes\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f\"Number of Classes: {NUM_CLASSES}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes: 5\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4b3384b9",
   "metadata": {},
   "source": [
    "# CELL 3 - CNN Model Buildup"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0d8ee8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:14:02.497350Z",
     "start_time": "2025-12-12T17:14:02.473690200Z"
    }
   },
   "source": [
    "class FruitCNN(nn.Module):\n",
    "    def __init__(self, num_conv_layers, filters, dense_units, dropout_rate, num_classes):\n",
    "        super(FruitCNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        current_img_size = IMAGE_SIZE\n",
    "\n",
    "        # --- Create Exactly 'num_conv_layers' (7) ---\n",
    "        for i in range(num_conv_layers):\n",
    "            out_channels = filters * (2**i)\n",
    "            # Cap filters to 512 to prevent memory explosion\n",
    "            if out_channels > 512: out_channels = 512\n",
    "\n",
    "            # 1. Convolution + Activation (Always add these)\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU()\n",
    "            ]\n",
    "\n",
    "            # 2. Conditional Pooling (Only pool if image is large enough)\n",
    "            # We need at least 2x2 pixels to pool. We use a buffer of 4.\n",
    "            if current_img_size > 4:\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "                current_img_size //= 2  # Track size reduction\n",
    "\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate flatten size dynamically\n",
    "        # This prevents the \"size mismatch\" error at the Linear layer\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "            feature_map = self.features(dummy)\n",
    "            flattened_size = feature_map.view(1, -1).size(1)\n",
    "\n",
    "        # Classifier Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(flattened_size, dense_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dense_units, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "157cb67c",
   "metadata": {},
   "source": [
    "# CELL 4 - Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "id": "706ed3aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:40:09.100793900Z",
     "start_time": "2025-12-12T17:14:08.505507100Z"
    }
   },
   "source": [
    "# --- Trainer Function for Optuna (One Epoch) ---\n",
    "def train_one_epoch_optuna(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Trains the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# --- Evaluator Function for Optuna ---\n",
    "def evaluate_model_optuna(model, val_loader, device):\n",
    "    \"\"\"Evaluates the model on validation data.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# --- Optuna Objective Function ---\n",
    "def objective(trial):\n",
    "    # --- Hyperparameters ---\n",
    "    # FIXED: Constraint set to exactly 7 layers as requested\n",
    "    hp_num_conv_layers = 7\n",
    "\n",
    "    # Search space for other parameters\n",
    "    hp_filters = trial.suggest_categorical('filters', [64, 128, 256]) # Start smaller to save RAM\n",
    "    hp_dense_units = trial.suggest_categorical('dense_units', [256, 512, 1024])\n",
    "    hp_dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.3, step=0.1)\n",
    "\n",
    "    # FIXED: Replaced deprecated suggest_loguniform with suggest_float(log=True)\n",
    "    hp_learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    print(f\"\\n--- Trial {trial.number} ---\")\n",
    "    print(f\"Params: Layers={hp_num_conv_layers}, Filters={hp_filters}, LR={hp_learning_rate:.6f}\")\n",
    "\n",
    "    # --- Model Setup ---\n",
    "    model = FruitCNN(\n",
    "        num_conv_layers=hp_num_conv_layers,\n",
    "        filters=hp_filters,\n",
    "        dense_units=hp_dense_units,\n",
    "        dropout_rate=hp_dropout_rate,\n",
    "        num_classes=NUM_CLASSES # Ensure this is defined in your global scope\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hp_learning_rate)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(NUM_EPOCHS_TUNE):\n",
    "        # Using your existing train/eval functions\n",
    "        train_loss = train_one_epoch_optuna(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        accuracy = evaluate_model_optuna(model, val_loader, DEVICE)\n",
    "\n",
    "        # Report & Prune\n",
    "        trial.report(accuracy, epoch)\n",
    "        if trial.should_prune():\n",
    "            print(f\"Trial {trial.number} pruned at epoch {epoch+1}\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# --- Run Optuna Search ---\n",
    "print(\"Starting Hyperparameter Optimization (HPO) with Optuna...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"\\n--- Best Trial Results ---\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best Validation Accuracy: {best_trial.value:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Retrieve best hyperparameters\n",
    "best_hps = best_trial.params"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:14:08,512] A new study created in memory with name: no-name-b59605f6-1bf2-4b49-b253-3cc934f25ecf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hyperparameter Optimization (HPO) with Optuna...\n",
      "\n",
      "--- Trial 0 ---\n",
      "Params: Layers=7, Filters=256, LR=0.000341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:18:49,391] Trial 0 finished with value: 0.5190721649484537 and parameters: {'filters': 256, 'dense_units': 1024, 'dropout_rate': 0.2, 'learning_rate': 0.0003407490778002714}. Best is trial 0 with value: 0.5190721649484537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 1 ---\n",
      "Params: Layers=7, Filters=256, LR=0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:23:25,520] Trial 1 finished with value: 0.43917525773195876 and parameters: {'filters': 256, 'dense_units': 256, 'dropout_rate': 0.3, 'learning_rate': 1.149025250465492e-05}. Best is trial 0 with value: 0.5190721649484537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 2 ---\n",
      "Params: Layers=7, Filters=64, LR=0.000081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:25:31,210] Trial 2 finished with value: 0.5278350515463918 and parameters: {'filters': 64, 'dense_units': 512, 'dropout_rate': 0.3, 'learning_rate': 8.137088230275953e-05}. Best is trial 2 with value: 0.5278350515463918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 3 ---\n",
      "Params: Layers=7, Filters=128, LR=0.000022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:28:12,337] Trial 3 finished with value: 0.44484536082474224 and parameters: {'filters': 128, 'dense_units': 256, 'dropout_rate': 0.3, 'learning_rate': 2.1680990231770377e-05}. Best is trial 2 with value: 0.5278350515463918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 4 ---\n",
      "Params: Layers=7, Filters=256, LR=0.000275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:32:09,886] Trial 4 finished with value: 0.5201030927835052 and parameters: {'filters': 256, 'dense_units': 256, 'dropout_rate': 0.2, 'learning_rate': 0.0002746873593053319}. Best is trial 2 with value: 0.5278350515463918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Trial 5 ---\n",
      "Params: Layers=7, Filters=256, LR=0.000171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:36:01,674] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 pruned at epoch 5\n",
      "\n",
      "--- Trial 6 ---\n",
      "Params: Layers=7, Filters=64, LR=0.000024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:36:26,453] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 pruned at epoch 1\n",
      "\n",
      "--- Trial 7 ---\n",
      "Params: Layers=7, Filters=128, LR=0.000056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:38:32,198] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 pruned at epoch 4\n",
      "\n",
      "--- Trial 8 ---\n",
      "Params: Layers=7, Filters=64, LR=0.000015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:39:20,255] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 pruned at epoch 2\n",
      "\n",
      "--- Trial 9 ---\n",
      "Params: Layers=7, Filters=64, LR=0.000152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 18:40:09,087] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 pruned at epoch 2\n",
      "\n",
      "--- Best Trial Results ---\n",
      "Best Validation Accuracy: 0.5278\n",
      "Best Hyperparameters:\n",
      "  filters: 64\n",
      "  dense_units: 512\n",
      "  dropout_rate: 0.3\n",
      "  learning_rate: 8.137088230275953e-05\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "16832e95",
   "metadata": {},
   "source": [
    "# CELL 5 - Training, Testing, Validation and Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "56c24d2e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-12T16:22:20.926717700Z"
    }
   },
   "source": [
    "# --- Final Model Initialization ---\n",
    "final_model = FruitCNN(\n",
    "    num_conv_layers=best_hps['num_conv_layers'],\n",
    "    filters=best_hps['filters'],\n",
    "    dense_units=best_hps['dense_units'],\n",
    "    dropout_rate=best_hps['dropout_rate'],\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=1e-2) #best_hps['learning_rate']\n",
    "\n",
    "# --- Custom Early Stopping Logic (Low-Level Logic) ---\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs, device, patience=10):\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    print(\"Starting Final Training with Early Stopping...\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            # FIX: labels already contains the class indices\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                # FIX: labels already contains the class indices\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        # Log History\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch}/{epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "        \n",
    "        # Early Stopping Logic \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best weights\n",
    "            best_model_weights = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Validation loss did not improve for {patience} epochs.\")\n",
    "                # Restore best weights\n",
    "                model.load_state_dict(best_model_weights)\n",
    "                break\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# --- Run Final Training ---\n",
    "final_model, history = train_and_validate(\n",
    "    final_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    NUM_EPOCHS_FINAL, \n",
    "    DEVICE\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Final Training with Early Stopping...\n",
      "Epoch 1/50: Train Loss: 28.4450, Val Loss: 1.6095, Val Acc: 0.2082\n",
      "Epoch 2/50: Train Loss: 1.6101, Val Loss: 1.6099, Val Acc: 0.1912\n",
      "Epoch 3/50: Train Loss: 1.6099, Val Loss: 1.6096, Val Acc: 0.1912\n",
      "Epoch 4/50: Train Loss: 1.6103, Val Loss: 1.6103, Val Acc: 0.1912\n",
      "Epoch 5/50: Train Loss: 1.6106, Val Loss: 1.6098, Val Acc: 0.1979\n",
      "Epoch 6/50: Train Loss: 1.6100, Val Loss: 1.6096, Val Acc: 0.1964\n",
      "Epoch 7/50: Train Loss: 1.6100, Val Loss: 1.6096, Val Acc: 0.2082\n",
      "Epoch 8/50: Train Loss: 1.6101, Val Loss: 1.6100, Val Acc: 0.1912\n",
      "Epoch 9/50: Train Loss: 1.6103, Val Loss: 1.6096, Val Acc: 0.1964\n",
      "Epoch 10/50: Train Loss: 1.6102, Val Loss: 1.6094, Val Acc: 0.2082\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e59c974",
   "metadata": {},
   "source": [
    "# CELL 6 - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df96b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Evaluation on Test Set ---\n",
    "def evaluate_test(model, test_loader, device, class_names):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # Labels need to be indices\n",
    "            inputs, labels = inputs.to(device), labels.to(device).argmax(dim=1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    # Calculate Overall Accuracy Score\n",
    "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nOverall Test Accuracy Score: {overall_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Classification Report (F-Score, Recall, Precision)\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    # Confusion Matrix (Visualization)\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return overall_accuracy\n",
    "\n",
    "# --- Run Test Evaluation ---\n",
    "test_accuracy = evaluate_test(final_model, test_loader, DEVICE, CLASS_NAMES)\n",
    "\n",
    "\n",
    "# --- Plot Training History ---\n",
    "# Plot Training History (Visualization)\n",
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    ax[0].plot(history['train_loss'], label='Train Loss')\n",
    "    ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    ax[0].set_title('Training and Validation Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax[1].plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- Training History Visualization ---\")\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CNN Model Definition ---\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        # Flatten dimension calculation: 100 -> 50 -> 25 -> 12 (int(12.5) -> 12)\n",
    "        # 128 channels * 12 * 12\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 12 * 12, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "print(\"Model Architecture Defined.\")\n"
   ],
   "id": "1751da10e7710dab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training and Validation Functions ---\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    return running_loss / len(loader), 100 * correct / total\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return running_loss / len(loader), 100 * correct / total\n"
   ],
   "id": "5e72ef7b1b362cf3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameter Optimization (Optuna) ---\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "    \n",
    "    # Instantiate model\n",
    "    model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "    # Training loop for HPO\n",
    "    for epoch in range(NUM_EPOCHS_TUNE):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "        \n",
    "        # Pruning\n",
    "        trial.report(val_acc, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "    return val_acc\n",
    "\n",
    "print(\"Starting Optuna Study...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5) # 5 trials for quick demonstration\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "    \n",
    "BEST_PARAMS = trial.params\n"
   ],
   "id": "d9337b6bf49d88b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Model Training ---\n",
    "\n",
    "print(\"Training Final Model with Best Parameters...\")\n",
    "\n",
    "final_model = SimpleCNN(NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = BEST_PARAMS['lr']\n",
    "if BEST_PARAMS['optimizer'] == 'Adam':\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = optim.SGD(final_model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_FINAL):\n",
    "    train_loss, train_acc = train_epoch(final_model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = validate(final_model, val_loader, criterion)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS_FINAL} | Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(final_model.state_dict(), 'best_fruit_model.pth')\n",
    "        \n"
   ],
   "id": "379b6130c639f407"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Training Results ---\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "plt.show()\n"
   ],
   "id": "a27024b171a4dc31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Set Evaluation ---\n",
    "\n",
    "# Load best model weights\n",
    "final_model.load_state_dict(torch.load('best_fruit_model.pth'))\n",
    "final_model.eval()\n",
    "\n",
    "test_loss, test_acc = validate(final_model, test_loader, criterion)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Confusion Matrix and Report\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = final_model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Display Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))\n"
   ],
   "id": "a70818b240106339"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fruit_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
